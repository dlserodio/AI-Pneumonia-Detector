{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU70kCGvkvnJ"
      },
      "source": [
        "# **Setting Up Environment and Libraries**\n",
        "In this section, we'll import the necessary libraries and set up the environment for our image classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jXlkzOskHWR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxPH-3fPlxGd"
      },
      "source": [
        "# **Data Preprocessing Class**\n",
        "In this section, we define a class DataPreprocessing to handle the preprocessing of our image data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fB7tdH5LkOIV"
      },
      "outputs": [],
      "source": [
        "class DataPreprocessing():\n",
        "    def __init__(self, directory_path, target_size=(224, 224), batch_size=32):\n",
        "        self.directory_path = directory_path\n",
        "        self.target_size = target_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def generate_datasets(self):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            validation_split=0.25\n",
        "\n",
        "        )\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            self.directory_path,\n",
        "            target_size=self.target_size,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode='categorical',\n",
        "            subset='training',\n",
        "            color_mode='grayscale',\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        validation_generator = train_datagen.flow_from_directory(\n",
        "            self.directory_path,\n",
        "            target_size=self.target_size,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode='categorical',\n",
        "            subset='validation',\n",
        "            color_mode='grayscale',\n",
        "            interpolation='bilinear',\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        return train_generator, validation_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRXDGg3RER9Z"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fqPrS-kkYgZ"
      },
      "outputs": [],
      "source": [
        "class ModelCreation():\n",
        "    def __init__(self, input_shape, num_classes, l2_weight=0.03, l1_weight=0.03):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.l2_weight = l2_weight\n",
        "        self.l1_weight = l1_weight\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential([\n",
        "            Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape,\n",
        "                   kernel_regularizer=regularizers.l2(self.l2_weight),\n",
        "                   activity_regularizer=regularizers.l1(self.l1_weight)),\n",
        "            MaxPooling2D((2, 2)),\n",
        "            Conv2D(64, (3, 3), activation='relu',\n",
        "                   kernel_regularizer=regularizers.l2(self.l2_weight),\n",
        "                   activity_regularizer=regularizers.l1(self.l1_weight)),\n",
        "            MaxPooling2D((2, 2)),\n",
        "            Conv2D(128, (3, 3), activation='relu',\n",
        "                   kernel_regularizer=regularizers.l2(self.l2_weight),\n",
        "                   activity_regularizer=regularizers.l1(self.l1_weight)),\n",
        "            MaxPooling2D((2, 2)),\n",
        "            Flatten(),\n",
        "            Dense(128, activation='relu',\n",
        "                  kernel_regularizer=regularizers.l2(self.l2_weight),\n",
        "                  activity_regularizer=regularizers.l1(self.l1_weight)),\n",
        "            Dropout(0.5),\n",
        "            Dense(self.num_classes, activation='softmax')\n",
        "        ])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5M-E6ahke4Y"
      },
      "outputs": [],
      "source": [
        "class ModelEvaluation:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def plot_learning_curves(self, history):\n",
        "        plt.plot(history.history['accuracy'], label='accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.ylim([0, 1])\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(history.history['loss'], label='loss')\n",
        "        plt.plot(history.history['val_loss'], label='val_loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_feature_maps(self, model, image):\n",
        "        # Create a model that outputs the activations of all convolutional layers\n",
        "        layer_outputs = [layer.output for layer in model.layers if isinstance(layer, tf.keras.layers.Conv2D)]\n",
        "        activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "        # Get the activations for the input image\n",
        "        activations = activation_model.predict(np.expand_dims(image, axis=0))\n",
        "\n",
        "        # Plot the activations for each layer\n",
        "        for i, activation in enumerate(activations):\n",
        "            num_features = activation.shape[-1]\n",
        "            size = activation.shape[1]\n",
        "            num_cols = min(num_features, 4)\n",
        "            num_rows = num_features // num_cols\n",
        "\n",
        "            plt.figure(figsize=(size, size))\n",
        "            for j in range(num_features):\n",
        "                ax = plt.subplot(num_rows, num_cols, j + 1)\n",
        "                ax.set_xticks([])\n",
        "                ax.set_yticks([])\n",
        "                ax.imshow(activation[0, :, :, j], cmap='viridis')\n",
        "            plt.show()\n",
        "\n",
        "    def plot_roc_curve(self, model, test_data):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        # Predict probabilities for test data\n",
        "        for x, y in test_data:\n",
        "            y_true.extend(np.argmax(y, axis=1))\n",
        "            y_pred.extend(np.argmax(model.predict(x), axis=1))\n",
        "\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        for i in range(len(np.unique(y_true))):\n",
        "            fpr[i], tpr[i], _ = roc_curve(np.array(y_true) == i, np.array(y_pred) == i)\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Plot ROC curve for each class\n",
        "        plt.figure()\n",
        "        colors = ['blue', 'red', 'green']  # Add more colors if needed\n",
        "        for i, color in zip(range(len(np.unique(y_true))), colors):\n",
        "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                     label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                     ''.format(i, roc_auc[i]))\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "nTXITXNbh-HH",
        "outputId": "9be608c1-dcf8-4e13-898e-0eec4940dbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1474 images belonging to 3 classes.\n",
            "Found 490 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7e9554473c74>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-7e9554473c74>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Specify the directory in Google Drive to save the model file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    data_dir = \"/content/drive/MyDrive/AI Pneumonia Detector/chest_xrays\"\n",
        "    input_shape = (224, 224, 1)\n",
        "    num_classes = 3\n",
        "\n",
        "    data_preprocessor = DataPreprocessing(data_dir)\n",
        "    train_data, validation_data = data_preprocessor.generate_datasets()\n",
        "\n",
        "    model_creator = ModelCreation(input_shape, num_classes)\n",
        "    model = model_creator.create_model()\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Specify the directory in Google Drive to save the model file\n",
        "    model_save_path = \"/content/drive/My Drive/AI Pneumonia Detector/best_model2.h5\"\n",
        "\n",
        "    # Configure ModelCheckpoint to save the best model to Google Drive\n",
        "    checkpoint = ModelCheckpoint(model_save_path, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(train_data,\n",
        "                        epochs=20,\n",
        "                        validation_data=validation_data,\n",
        "                        callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "    evaluation = ModelEvaluation()\n",
        "    # Validation and Loss Curves plot for train and validation\n",
        "    evaluation.plot_learning_curves(history)\n",
        "    # ROC Curve plot for the validation data\n",
        "    evaluation.plot_roc_curve(model, validation_data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qp12SIDzpTdFO8hDP-zsqp3t8loMiQ6u",
      "authorship_tag": "ABX9TyPTnEWUTa239nhnq/mTPoxm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}